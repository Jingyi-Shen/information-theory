# information-theory-in-ML

## deep learning for statistical inference
* Conditional Independence Testing
 * Estimating total variation distance between two distribution
 * Distance measure estimation using classifier

* Estimating Information
 * Estimating Kullback-Leibler Distance
 * Information bottleneck and deep learning
 * Conditional mutual information estimation (Plays vital role in controlling bias or privacy)


## Information theory for deep learning
* GAN based generative models
 * Parametric models (e.g. mixture of Gaussians) fail on complex data
 * Non-parametric models (e.g. KDE, Nearest Neighbor) fail in high dimensions

* Learning Gated Neural Networks
 * GAN loss: Cross-Entropy loss; f-divergence; Wasserstein distance 
  * Mode Collapse collectively refers to the lack of diversity in the generated samples. \<br>
  * One weakness of GAN is that the latent variable Z has no interpretable meaning. \<br>
